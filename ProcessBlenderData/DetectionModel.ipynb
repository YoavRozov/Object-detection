{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9835edc",
   "metadata": {},
   "source": [
    "## 1. Setting Up Blender\n",
    "To begin, ensure you have Blender installed on your system. You can download it from [blender.org](https://www.blender.org/download/).\n",
    "\n",
    "Download or create your own HDRI's (environment textures) to use inside Blender for random backgrounds.\n",
    "[Download Free, CC0, High Resolution HDRI's](https://polyhaven.com/hdris) (Use HDR format)\n",
    "\n",
    "Additionally, set up your Python environment with necessary libraries such as `cv2`, `ultralytics`.\n",
    "\n",
    "\n",
    "Finlay If you have a dedicated GPU in your system, make sure to set up Blender and pytorch to use it for rendering and model training.\n",
    "\n",
    "Use the cell below to check your environment and make sure all the packages are working properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: Run this in a Jupyter environment with internet access\n",
    "\n",
    "# YOLOv11 (Ultralytics)\n",
    "%pip install ultralytics --quiet\n",
    "\n",
    "# OpenCV for image processing\n",
    "%pip install opencv-python --quiet\n",
    "\n",
    "\n",
    "# Install torch with CUDA support if a compatible GPU is available\n",
    "# https://pytorch.org/get-started/locally/\n",
    "\n",
    "\n",
    "# Blender's Python API is included in Blender itself.\n",
    "# If you want to run Blender scripts from outside Blender, you need to call Blender in background mode:\n",
    "# blender --background --python your_script.py\n",
    "\n",
    "# Check versions\n",
    "import sys\n",
    "import cv2\n",
    "import ultralytics\n",
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"Ultralytics YOLO version:\", ultralytics.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available for PyTorch training\n",
    "# This is important for performance, especially with large models and datasets\n",
    "# For more information, visit: https://pytorch.org/docs/stable/notes/cuda.html\n",
    "# If you encounter issues, ensure that your CUDA drivers are correctly installed and compatible with your PyTorch version.\n",
    "print(\"Is GPU available for PyTorch:\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766f0b6",
   "metadata": {},
   "source": [
    "## 2. Convert Images to Grayscale and organize them for YOLO\n",
    "To force the model to rely less on color data, we can convert the rendered RGB images to grayscale using OpenCV.\n",
    "\n",
    "Use the cell below to perform the conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing image data from Blender renders\n",
    "image_data_folder = \"Path/To/Blender/Output/Folder/Images\"\n",
    "\n",
    "# Output folder for grayscale images\n",
    "output_folder = \"Path/To/Folder/Where/To/Store/Training/Data\"\n",
    "\n",
    "# Create output folder and subfolders if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"Train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"Val\"), exist_ok=True)\n",
    "\n",
    "# Convert all images in the image data folder to grayscale and save to output folder, train, and val subfolders\n",
    "image_files = [f for f in os.listdir(image_data_folder) if f.lower().endswith('.jpg')]\n",
    "image_files.sort()  # for reproducibility\n",
    "\n",
    "# Split into training and validation sets (90% train, 20% val)\n",
    "num_val = max(1, int(0.2 * len(image_files)))\n",
    "val_files = image_files[:num_val]\n",
    "train_files = image_files[num_val:]\n",
    "\n",
    "train_folder = os.path.join(output_folder, \"Train\")\n",
    "val_folder = os.path.join(output_folder, \"Val\")\n",
    "\n",
    "for fname in train_files:\n",
    "    img = cv2.imread(os.path.join(image_data_folder, fname))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(os.path.join(train_folder, fname), gray)\n",
    "\n",
    "for fname in val_files:\n",
    "    img = cv2.imread(os.path.join(image_data_folder, fname))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(os.path.join(val_folder, fname), gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12114d4",
   "metadata": {},
   "source": [
    "## 3. Generating Annotations in YOLOv11 Format\n",
    "Once images are rendered, we need to generate bounding box annotations in YOLOv11 format. This format includes class ID and normalized coordinates for each object.\n",
    "\n",
    "Use the cell below to generate annotation files.\n",
    "\n",
    "In this example we generate annotation files for YOLOv11 [Object Detection](https://docs.ultralytics.com/tasks/detect/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/val split from grayscale images\n",
    "train_files = set(os.listdir(train_folder))\n",
    "val_files = set(os.listdir(val_folder))\n",
    "\n",
    "# Define range of red color in HSV\n",
    "lower_red = np.array([0, 50, 50])\n",
    "upper_red = np.array([10, 255, 255])\n",
    "\n",
    "mask_images_folder = \"Path/To/Blender/Output/Folder/Masks\"\n",
    "\n",
    "for filename in os.listdir(mask_images_folder):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        img = cv2.imread(os.path.join(mask_images_folder, filename))\n",
    "\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        # Convert BGR to HSV\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Threshold the HSV image to get only red colors\n",
    "        mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "\n",
    "        # Find contours in the binary image\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # This assumes the largest contour is the object of interest and you only have one object per image\n",
    "        if contours:\n",
    "            contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "            # Find the bounding box of the largest contour\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "            # Calculate normalized bounding box coordinates\n",
    "            x_center = (x + w/2) / img.shape[1]\n",
    "            y_center = (y + h/2) / img.shape[0]\n",
    "            width = w / img.shape[1]\n",
    "            height = h / img.shape[0]\n",
    "\n",
    "            # Draw the contour on the original image for visualization\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.imshow('Contour', img)\n",
    "            cv2.waitKey(2)  # Display each image for 2 ms\n",
    "\n",
    "            # Save annotation in YOLOv11 format\n",
    "            txt_folder = \"\"\n",
    "            # Determine if the image is in the training or validation set\n",
    "            if filename in train_files:\n",
    "                txt_folder = train_folder\n",
    "            elif filename in val_files:\n",
    "                txt_folder = val_folder\n",
    "            \n",
    "            with open(os.path.join(txt_folder, filename.replace('.jpg', '.txt')), 'w') as f:\n",
    "                # Class ID is 0 for single-class segmentation\n",
    "                class_id = 0\n",
    "                \n",
    "                # Write class ID and points to file\n",
    "                # YOLOv11 instance segmentation format: class_id x_center y_center width height\n",
    "                f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Create YOLOv11 YAML configuration file, change the name of the class if needed (from coral to algae, cone, etc.)\n",
    "yaml_content = f\"\"\"\n",
    "train: {train_folder}\n",
    "val: {val_folder}\n",
    "nc: 1\n",
    "names: ['coral']\n",
    "\"\"\"\n",
    "\n",
    "yaml_path = os.path.join(output_folder, \"yolov11_config.yaml\")\n",
    "with open(yaml_path, 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"YOLOv11 configuration file saved to {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f33aafb",
   "metadata": {},
   "source": [
    "## 5. Training YOLOv11 Object Detection Model\n",
    "With the dataset and annotations ready, we can now train a YOLOv11 object detection model. This step involves configuring the model, loading the dataset, and running the training process.\n",
    "\n",
    "Use the cell below to initiate training.\n",
    "\n",
    "For more info visit the [ultralytics website](https://docs.ultralytics.com/tasks/detect/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5735ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"  # device=0 for GPU, \"cpu\" for CPU\n",
    "results = model.train(data=yaml_path, epochs=400, imgsz=640, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589718a",
   "metadata": {},
   "source": [
    "## 6. Using the Trained Model for Detection\n",
    "After training, we can use the model to detect objects in new images. This step involves loading the trained weights and running predictions on test images.\n",
    "\n",
    "Use the cell below to perform prediction on test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom model\n",
    "model = YOLO(\"runs/detect/train/weights/best.pt\")\n",
    "\n",
    "# Load test image\n",
    "test_image = cv2.imread(\"Path/to/image.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "test_image = cv2.cvtColor(test_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# Use the custom model\n",
    "model.predict(source=test_image, save=True, show=True, conf=0.1, save_txt=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
