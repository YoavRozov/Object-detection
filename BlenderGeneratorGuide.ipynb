{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fd7ec0",
   "metadata": {},
   "source": [
    "# Object Detection with Synthetic Data using Blender and YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ada1df",
   "metadata": {},
   "source": [
    "## 1. Setting Up Blender\n",
    "To begin, ensure you have Blender installed on your system. You can download it from [blender.org](https://www.blender.org/download/).\n",
    "\n",
    "Download or create your own HDRI's (environment textures) to use inside Blender for random backgrounds.\n",
    "[Download Free, CC0, High Resolution HDRI's](https://polyhaven.com/hdris) (Use HDR format)\n",
    "\n",
    "Additionally, set up your Python environment with necessary libraries such as `cv2`, `ultralytics`.\n",
    "\n",
    "\n",
    "Finlay If you have a dedicated GPU in your system, make sure to set up Blender and pytorch to use it for rendering and model training.\n",
    "\n",
    "Use the cell below to check your environment and make sure all the packages are working properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcdeb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Note: Run this in a Jupyter environment with internet access\n",
    "\n",
    "# YOLOv11 (Ultralytics)\n",
    "%pip install ultralytics --quiet\n",
    "\n",
    "# OpenCV for image processing\n",
    "%pip install opencv-python --quiet\n",
    "\n",
    "\n",
    "# Install torch with CUDA support if a compatible GPU is available\n",
    "# https://pytorch.org/get-started/locally/\n",
    "\n",
    "\n",
    "# Blender's Python API is included in Blender itself.\n",
    "# If you want to run Blender scripts from outside Blender, you need to call Blender in background mode:\n",
    "# blender --background --python your_script.py\n",
    "\n",
    "# Check versions\n",
    "import sys\n",
    "import cv2\n",
    "import ultralytics\n",
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"OpenCV version:\", cv2.__version__)\n",
    "print(\"Ultralytics YOLO version:\", ultralytics.__version__)\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c24156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available for PyTorch training\n",
    "# This is important for performance, especially with large models and datasets\n",
    "# For more information, visit: https://pytorch.org/docs/stable/notes/cuda.html\n",
    "# If you encounter issues, ensure that your CUDA drivers are correctly installed and compatible with your PyTorch version.\n",
    "print(\"Is GPU available for PyTorch:\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3173d0e",
   "metadata": {},
   "source": [
    "### Open the Blender file that is included in this repository: `DatasetGenerator.blend`\n",
    "It includes a Coral and an Algae model (By default only the Coral is visible, if you wish to create a dataset for the algae game piece, hide the coral from the viewport, disable it in the render and enable the algae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236f93a",
   "metadata": {},
   "source": [
    "## 2. Automating Blender for Synthetic Data Generation\n",
    "\n",
    "Using the **Blender's Python API** I automate the rendering of synthetic images.  \n",
    "This includes randomizing environments, objects, and camera angles to generate diverse training data for machine learning applications.\n",
    "<br>\n",
    "\n",
    "### <span style=\"color:red\">Important Notice</span>\n",
    "\n",
    "Before running the automation script and rendering the animation, make sure the following settings are correctly configured in Blender:\n",
    "\n",
    "- **Render Engine:** Set to `Cycles`\n",
    "- **Device:** Set to `GPU Compute`  \n",
    "  *(If it it is gray go to: `Edit` → `Preferences` → `System` → `Cycles Render Devices`)*\n",
    "\n",
    "<img src=\"media/System.png\" alt=\"System Preferences\" width=\"650\"/>\n",
    "<img src=\"media/RenderPreferences.png\" alt=\"Render Preferences\" width=\"480\"/>\n",
    "\n",
    "- **Output Settings:**\n",
    "  - Choose the correct **output folder**\n",
    "  - The **Frame Range** defines how much images you will get\n",
    "  - Set **file format** to `JPEG`\n",
    "\n",
    "<img src=\"media/OutputPreferences.png\" alt=\"Output Preferences\" width=\"400\"/>\n",
    "\n",
    "---\n",
    "\n",
    "### **<span style=\"color:green\">Run the script and Render the animation</span>**\n",
    "To run the script navigate to the `Scripting` tab and press *Run Script*.\n",
    "\n",
    "Before running the script make sure the `hdr_folder` path is specified correctly, you can download HDRI's [here](https://polyhaven.com/hdris).\n",
    "\n",
    "After running the script, scroll through the timeline at the bottom and check that the camera is moving as expected.\n",
    "\n",
    "Finally press `Render` → `Render Animation`.\n",
    "\n",
    "Be sure to configure your scene as described above before rendering.\n",
    "\n",
    "<img src=\"media/RunScript.png\" alt=\"Output Preferences\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a138eee",
   "metadata": {},
   "source": [
    "After generating the Image data, go to render properties and change the `max samples` under `Sampling` → `Render` to 1, and under `Film` check the `Transparent` option\n",
    "\n",
    "Then change both materials of the Coral Object to the `RED` material, then delete the `Sphere` object\n",
    "\n",
    "Finally, select a different output folder and render the animation to get the masks for your Image data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f875200",
   "metadata": {},
   "source": [
    "## 3. Convert Images to Grayscale and organize them for YOLO\n",
    "To force the model to rely less on color data, we can convert the rendered RGB images to grayscale using OpenCV or similar libraries.\n",
    "\n",
    "Use the cell below to perform the conversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder containing image data from Blender renders\n",
    "image_data_folder = \"Path/To/Blender/Output/Folder/Images\"\n",
    "\n",
    "# Output folder for grayscale images\n",
    "output_folder = \"Path/To/Folder/Where/To/Store/Training/Data\"\n",
    "\n",
    "# Create output folder and subfolders if they don't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"Train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_folder, \"Val\"), exist_ok=True)\n",
    "\n",
    "# Convert all images in the image data folder to grayscale and save to output folder, train, and val subfolders\n",
    "image_files = [f for f in os.listdir(image_data_folder) if f.lower().endswith('.jpg')]\n",
    "image_files.sort()  # for reproducibility\n",
    "\n",
    "# Split into training and validation sets (90% train, 20% val)\n",
    "num_val = max(1, int(0.2 * len(image_files)))\n",
    "val_files = image_files[:num_val]\n",
    "train_files = image_files[num_val:]\n",
    "\n",
    "for fname in train_files:\n",
    "    img = cv2.imread(os.path.join(image_data_folder, fname))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(os.path.join(output_folder, \"Train\", fname), gray)\n",
    "\n",
    "for fname in val_files:\n",
    "    img = cv2.imread(os.path.join(image_data_folder, fname))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imwrite(os.path.join(output_folder, \"Val\", fname), gray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
